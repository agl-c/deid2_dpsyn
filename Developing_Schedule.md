# Developing Schedule
Note that git seetings to check about password login
## we're in week-5 (coding-style should be handled and Configration may be tackled)
1. PS: despite that I reviewed the backbone of PrivSyn, as to specific coding logic,
still hard work is needed to check 

you can do the following things simutaneously : 
read and understand,
add docstring / comments 
run pylint  to recheck
2. I'm thinking about how to set interface for configration  (maybe read others' repo and ask people for help) 
PS: As to generate the attached data parameters and data type analysis, I quite guess these are generated by programmes instead of manual human work? ğŸ¤£


Q: what the difference between data.yaml & data_no_encode.yaml?


3. and I worry about generalization to some extent, maybe for I should consider the general performance later and do things step by step...
4. I want to run the code once to test dataset generation (even just on our groundtruth.csv and state-15-data.csv) 

5. As to promote the research, more related work on histogram publishing is required I think







weekly sync Link:
https://teams.microsoft.com/l/meetup-join/19%3ameeting_NzFjNmI1YWItYzMxOC00ZTYxLWExOGUtNzZkOGM4ZjI4NmJh%40thread.v2/0?context=%7b%22Tid%22%3a%222ab5d82f-d8fa-4797-a93e-054655c61dec%22%2c%22Oid%22%3a%221e8e3294-bdc8-4b0b-a61d-697689c9ea2a%22%7d
### received (have generally taken a look)
*general open-source repos on github*
*datasets to fit/ experiment/ generalize*

### on-coming:
**(little)possible similar dp open-source repos **  
   e.g. https://github.com/opendp/smartnoise-core-python (have taken a look, it actually is a paclage in python while inner program is in Rust language)


### Done Background Knowledge:
0.python style guide by Google/official
1.python *logger*
2.*pylint* ( give it a shot ) to check code standard things 
Qï¼šlogger.()ç›´æ¥è°ƒç”¨çš„è¯æ—¥å¿—ä¼šå»å“ªé‡Œï¼Œæ˜¯ä¸æ˜¯è·‘è¯¥.pyä¼šè¾“å‡ºå‘¢ï¼Œè¿˜æ˜¯è¯´å­˜åˆ°äº†æŸä¸ªlogæ–‡ä»¶
Aï¼šæŸ¥èµ„æ–™å‘ç°é»˜è®¤æ˜¯åœ¨è¿è¡Œæ—¶è¾“å‡ºåˆ°ç»ˆç«¯ï¼Œå¯ä»¥å¯¹basic settingåšå„ç§ä¿®æ”¹ï¼Œåªéœ€è¦åŠ ä¸Šä¸€ä¸¤å¥è¯å³å¯ï¼›
ä¾‹å¦‚ä¸ºè¾“å‡ºåˆ°æ–‡ä»¶æˆ–è€…æŸäº›destination
Q:  ä¸ºä»€ä¹ˆæˆ‘åœ¨æ³¨é‡Šé‡Œé¢åŠ optionéƒ½èƒ½è¯†åˆ«æœ‰ç”¨å‘¢
Aï¼šå¯èƒ½å’Œpylintè¿è¡Œæ—¶å€™çš„ç¼–è¯‘æœ‰å…³ï¼Œæ€»ä¹‹è¿™æ ·å¯ä»¥å¯¹è¯¥moduleä½œå‡ºæµ‹è¯•çš„optionç®¡ç†è¾“å‡ºä¿¡æ¯

3.yaml language specifications read
4.quick note about Package module to install by pip
5.learn the python package pandas in dealing with data
6.leran .csv file format
   
## for future possible use: 
0. sample/direct_sample/plain_pub(which use deepcopy, any polishing?)
1. <font color=red>generalization take care of fixed values:</font>>
   e.g.
    bias_penalty_cutoff = 250
   e.g.
    sensitivity = 'max_records_per_individual' which relies on the dataset settings
   e.g.
    post_processing (......)
    cases like puma_year_detailed......
## research thinking
refer to overleaf link




https://aws.amazon.com/s3/?did=ft_card&trk=ft_card
https://aws.amazon.com/sdk-for-python/
https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3-examples.html

08/11 meet notes
1. maybe on Nov.8th(date to sync via email spread sheet) they'll hold a virtual workshop where we 3 teams are required to present how our package works and the output of the challenges
2. they've provided more public safety datasets to test on certain point (despite the fact that some of them might have to be cleaned to test on)
3. as to dataset storage, they suggest use free-tier aws3, for which tool related tutorials are offered, too
4. next week they want us to discuss more on how we think about configration and perhaps show user experience on how it works
5. a metric related package is also offered to help measure the synthesized dataset, which link is also provided (seeming that some fellows we can ask for help if in need)
6. they mention that after we supply the final work, they might ask NIST for help to allocate a permanent space to store our repositories for contributing to open-source work